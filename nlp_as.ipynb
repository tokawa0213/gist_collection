{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "#nlp_as.txt\n",
        "#http://www.george-orwell.org/1984/0.html\n",
        "#DATA VISUALIZATION\n",
        "with open(\"nlp_as.txt\") as f:\n",
        "    for line in f.readlines()[:30]:    \n",
        "        print(line)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PART ONE\n",
            "\n\n\n\n\n\n\n",
            "Chapter 1\n",
            "\n\n\n\n\n\n\n",
            "It was a bright cold day in April, and the clocks were striking thirteen.\n",
            "\n",
            "Winston Smith, his chin nuzzled into his breast in an effort to escape the\n",
            "\n",
            "vile wind, slipped quickly through the glass doors of Victory Mansions,\n",
            "\n",
            "though not quickly enough to prevent a swirl of gritty dust from entering\n",
            "\n",
            "along with him.\n",
            "\n\n\n",
            "The hallway smelt of boiled cabbage and old rag mats. At one end of it a\n",
            "\n",
            "coloured poster, too large for indoor display, had been tacked to the wall.\n",
            "\n",
            "It depicted simply an enormous face, more than a metre wide: the face of a\n",
            "\n",
            "man of about forty-five, with a heavy black moustache and ruggedly handsome\n",
            "\n",
            "features. Winston made for the stairs. It was no use trying the lift. Even\n",
            "\n",
            "at the best of times it was seldom working, and at present the electric\n",
            "\n",
            "current was cut off during daylight hours. It was part of the economy drive\n",
            "\n",
            "in preparation for Hate Week. The flat was seven flights up, and Winston,\n",
            "\n",
            "who was thirty-nine and had a varicose ulcer above his right ankle, went\n",
            "\n",
            "slowly, resting several times on the way. On each landing, opposite the\n",
            "\n",
            "lift-shaft, the poster with the enormous face gazed from the wall. It was\n",
            "\n",
            "one of those pictures which are so contrived that the eyes follow you about\n",
            "\n",
            "when you move. BIG BROTHER IS WATCHING YOU, the caption beneath it ran.\n",
            "\n\n\n",
            "Inside the flat a fruity voice was reading out a list of figures which had\n",
            "\n",
            "something to do with the production of pig-iron. The voice came from an\n",
            "\n"
          ]
        }
      ],
      "execution_count": 24,
      "metadata": {
        "collapsed": true
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#(a)Numbre of word token and word type\n",
        "\n",
        "import re\n",
        "from collections import Counter\n",
        "\n",
        "with open(\"nlp_as.txt\") as f:\n",
        "    PATTERN = r'([^a-zA-Z0-9])'\n",
        "    WORD_SET = []\n",
        "    for line in f:\n",
        "        line = line.lower()\n",
        "        line = re.sub(PATTERN,r\" \\1 \",line)\n",
        "        WORD_SET.extend(line.split())\n",
        "WORD_COUNTER = Counter(WORD_SET)\n",
        "WORD_TOKEN = len(WORD_SET)\n",
        "WORD_TYPE = len(set(WORD_SET))\n",
        "print(\"WORD_TOKEN: \" + str(WORD_TOKEN))\n",
        "print(\"WORD_TYPE: \" + str(WORD_TYPE))"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WORD_TOKEN: 124236\n",
            "WORD_TYPE: 8878\n"
          ]
        }
      ],
      "execution_count": 81,
      "metadata": {
        "collapsed": false,
        "outputHidden": false,
        "inputHidden": false
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#(b) find all bigrams\n",
        "import nltk\n",
        "corpus = nltk.Text(WORD_SET)\n",
        "bigram = nltk.bigrams(corpus)\n",
        "cfd = nltk.ConditionalFreqDist(bigram)\n",
        "ALL_BIGRAMS = []\n",
        "BIGRAM_FREQUENCY = defaultdict(int)\n",
        "for first_word,second_word_dict in cfd.items():\n",
        "    for second_word,value in second_word_dict.items():\n",
        "        BIGRAM_FREQUENCY[first_word + \" \" + second_word] += value\n",
        "ALL_BIGRAMS = list(BIGRAM_FREQUENCY.keys())\n",
        "print(ALL_BIGRAMS[:30])\n",
        "BIGRAM_LENGTH = len(ALL_BIGRAMS)\n",
        "BIGRAM_SET_LENGTH = len(set(ALL_BIGRAMS))\n",
        "print(\"NUMBER OF BIGRAMS : \" + str(BIGRAM_LENGTH))"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['part one', 'part of', 'part ,', 'part in', 'part two', 'part .', 'part -', 'part three', 'one chapter', 'one end', 'one of', 'one on', 'one corner', 'one ,', 'one .', 'one had', 'one side', 'one very', \"one '\", 'one with', 'one should', 'one knew', 'one was', 'one even', 'one felt', 'one object', 'one moment', 'one wrenches', 'one seemed', 'one into']\n",
            "NUMBER OF BIGRAMS : 52048\n"
          ]
        }
      ],
      "execution_count": 111,
      "metadata": {
        "collapsed": false,
        "outputHidden": false,
        "inputHidden": false
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#(c) Percentage of actual bigram over possible bigram\n",
        "BIGRAM_LENGTH*100/(WORD_TYPE*WORD_TYPE)"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 58,
          "data": {
            "text/plain": [
              "0.06355821138731339"
            ]
          },
          "metadata": {}
        }
      ],
      "execution_count": 58,
      "metadata": {
        "collapsed": false,
        "outputHidden": false,
        "inputHidden": false
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#(d) Calculate the possibility of the sentence\n",
        "\n",
        "from collections import defaultdict\n",
        "#Known sentence\n",
        "sample_sentence =  \"it was one of those pictures which are so contrived that the eyes follow you about when you move .\"\n",
        "sample_sentence2 = \"it was part of the economy drive in preparation for hate week .\"\n",
        "#unknown sentence\n",
        "#Using Laplace's Rule\n",
        "sample_sentence3 = \"tokawa is a s student at a graduate school .\"\n",
        "sentences = [sample_sentence,sample_sentence2,sample_sentence3]\n",
        "#Calculating by unigrams\n",
        "\n",
        "WORD_TOKEN = float(WORD_TOKEN)+WORD_TYPE\n",
        "\n",
        "for sentence in sentences:\n",
        "    total = 1\n",
        "    print(\"WORD COUNT ALL_WORDS PERCENTAGE\")\n",
        "    for i in sentence.split():\n",
        "        print(i,WORD_COUNTER[i]+1,WORD_TOKEN,WORD_COUNTER[i]/WORD_TOKEN)\n",
        "        total *= (WORD_COUNTER[i]+1)/WORD_TOKEN\n",
        "    print()\n",
        "    print(\"Calulated sentence probability using unigram:\",total*100)\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WORD COUNT ALL_WORDS PERCENTAGE\n",
            "it 1923 186382.0 0.010312154607204558\n",
            "was 2319 186382.0 0.012436823298387183\n",
            "one 446 186382.0 0.002387569615091586\n",
            "of 3496 186382.0 0.018751810797179986\n",
            "those 54 186382.0 0.0002843622238198968\n",
            "pictures 5 186382.0 2.1461299910935604e-05\n",
            "which 475 186382.0 0.0025431640394458694\n",
            "are 299 186382.0 0.0015988668433647025\n",
            "so 211 186382.0 0.0011267182453241194\n",
            "contrived 7 186382.0 3.2191949866403405e-05\n",
            "that 1510 186382.0 0.008096275391400457\n",
            "the 6525 186382.0 0.035003380154735975\n",
            "eyes 123 186382.0 0.0006545696472835359\n",
            "follow 12 186382.0 5.901857475507292e-05\n",
            "you 1014 186382.0 0.005435074202444442\n",
            "about 155 186382.0 0.0008262600465710208\n",
            "when 321 186382.0 0.0017169039928748484\n",
            "you 1014 186382.0 0.005435074202444442\n",
            "move 19 186382.0 9.657584959921023e-05\n",
            ". 6105 186382.0 0.03274994366408773\n",
            "\n",
            "Calulated sentence probability using unigram: 1.3280418160407166e-54\n",
            "WORD COUNT ALL_WORDS PERCENTAGE\n",
            "it 1923 186382.0 0.010312154607204558\n",
            "was 2319 186382.0 0.012436823298387183\n",
            "part 42 186382.0 0.00021997832408708994\n",
            "of 3496 186382.0 0.018751810797179986\n",
            "the 6525 186382.0 0.035003380154735975\n",
            "economy 8 186382.0 3.755727484413731e-05\n",
            "drive 5 186382.0 2.1461299910935604e-05\n",
            "in 1871 186382.0 0.010033157708362396\n",
            "preparation 7 186382.0 3.2191949866403405e-05\n",
            "for 668 186382.0 0.003578671760148512\n",
            "hate 38 186382.0 0.00019851702417615435\n",
            "week 35 186382.0 0.00018242104924295265\n",
            ". 6105 186382.0 0.03274994366408773\n",
            "\n",
            "Calulated sentence probability using unigram: 3.7065961807730976e-36\n",
            "WORD COUNT ALL_WORDS PERCENTAGE\n",
            "tokawa 1 186382.0 0.0\n",
            "is 634 186382.0 0.0033962507109055596\n",
            "a 2578 186382.0 0.013826442467620263\n",
            "s 448 186382.0 0.002398300265047054\n",
            "student 1 186382.0 0.0\n",
            "at 668 186382.0 0.003578671760148512\n",
            "a 2578 186382.0 0.013826442467620263\n",
            "graduate 4 186382.0 1.6095974933201702e-05\n",
            "school 10 186382.0 4.8287924799605114e-05\n",
            ". 6105 186382.0 0.03274994366408773\n",
            "\n",
            "Calulated sentence probability using unigram: 6.087171575870361e-31\n"
          ]
        }
      ],
      "execution_count": 146,
      "metadata": {
        "collapsed": false,
        "outputHidden": false,
        "inputHidden": false
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import defaultdict\n",
        "#Known sentence\n",
        "sample_sentence =  \"it was one of those pictures which are so contrived that the eyes follow you about when you move .\"\n",
        "sample_sentence2 = \"it was part of the economy drive in preparation for hate week .\"\n",
        "#unknown sentence\n",
        "#Using Laplace's Rule\n",
        "sample_sentence3 = \"tokawa is a s student at a graduate school .\"\n",
        "sentences = [sample_sentence,sample_sentence2,sample_sentence3]\n",
        "\n",
        "BIGRAM_LENGTH = float(BIGRAM_LENGTH)+BIGRAM_SET_LENGTH\n",
        "\n",
        "for i in sentences:\n",
        "    i = \". \" + i\n",
        "    i = nltk.bigrams(i.split())\n",
        "\n",
        "    #Calculating by bigrams\n",
        "\n",
        "    print(\"BIGRAM COUNT ALL_BIGRAMS PERCENTAGE\")\n",
        "    total = 1\n",
        "    for word in i:\n",
        "        key = \" \".join(word)\n",
        "        print(key,BIGRAM_FREQUENCY[key]+1,BIGRAM_LENGTH,float(BIGRAM_FREQUENCY[key]+1)/BIGRAM_LENGTH)\n",
        "        total *= float(BIGRAM_FREQUENCY[key]+1)/BIGRAM_LENGTH\n",
        "    print()\n",
        "    print(\"Calulated sentence probability using bigram:\",total*100)\n",
        "    print()"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BIGRAM COUNT ALL_BIGRAMS PERCENTAGE\n",
            ". it 461 217070.0 0.002123738886073617\n",
            "it was 609 217070.0 0.0028055465978716544\n",
            "was one 16 217070.0 7.370894181600406e-05\n",
            "one of 62 217070.0 0.0002856221495370157\n",
            "of those 15 217070.0 6.91021329525038e-05\n",
            "those pictures 2 217070.0 9.213617727000507e-06\n",
            "pictures which 2 217070.0 9.213617727000507e-06\n",
            "which are 5 217070.0 2.3034044317501268e-05\n",
            "are so 2 217070.0 9.213617727000507e-06\n",
            "so contrived 2 217070.0 9.213617727000507e-06\n",
            "contrived that 2 217070.0 9.213617727000507e-06\n",
            "that the 148 217070.0 0.0006818077117980375\n",
            "the eyes 28 217070.0 0.0001289906481780071\n",
            "eyes follow 2 217070.0 9.213617727000507e-06\n",
            "follow you 2 217070.0 9.213617727000507e-06\n",
            "you about 2 217070.0 9.213617727000507e-06\n",
            "about when 2 217070.0 9.213617727000507e-06\n",
            "when you 31 217070.0 0.00014281107476850786\n",
            "you move 2 217070.0 9.213617727000507e-06\n",
            "move . 6 217070.0 2.764085318100152e-05\n",
            "\n",
            "Calulated sentence probability using bigram: 3.0558193467071737e-86\n",
            "\n",
            "BIGRAM COUNT ALL_BIGRAMS PERCENTAGE\n",
            ". it 461 217070.0 0.002123738886073617\n",
            "it was 609 217070.0 0.0028055465978716544\n",
            "was part 7 217070.0 3.224766204450177e-05\n",
            "part of 27 217070.0 0.00012438383931450683\n",
            "of the 776 217070.0 0.003574883678076197\n",
            "the economy 3 217070.0 1.382042659050076e-05\n",
            "economy drive 2 217070.0 9.213617727000507e-06\n",
            "drive in 2 217070.0 9.213617727000507e-06\n",
            "in preparation 4 217070.0 1.8427235454001014e-05\n",
            "preparation for 4 217070.0 1.8427235454001014e-05\n",
            "for hate 7 217070.0 3.224766204450177e-05\n",
            "hate week 10 217070.0 4.6068088635002536e-05\n",
            "week . 9 217070.0 4.146127977150228e-05\n",
            "\n",
            "Calulated sentence probability using bigram: 2.0964573921419497e-52\n",
            "\n",
            "BIGRAM COUNT ALL_BIGRAMS PERCENTAGE\n",
            ". tokawa 1 217070.0 4.606808863500254e-06\n",
            "tokawa is 1 217070.0 4.606808863500254e-06\n",
            "is a 32 217070.0 0.00014741788363200811\n",
            "a s 1 217070.0 4.606808863500254e-06\n",
            "s student 1 217070.0 4.606808863500254e-06\n",
            "student at 1 217070.0 4.606808863500254e-06\n",
            "at a 32 217070.0 0.00014741788363200811\n",
            "a graduate 1 217070.0 4.606808863500254e-06\n",
            "graduate school 1 217070.0 4.606808863500254e-06\n",
            "school . 1 217070.0 4.606808863500254e-06\n",
            "\n",
            "Calulated sentence probability using bigram: 4.40861215918952e-49\n",
            "\n"
          ]
        }
      ],
      "execution_count": 147,
      "metadata": {
        "collapsed": false,
        "outputHidden": false,
        "inputHidden": false
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": false,
        "outputHidden": false,
        "inputHidden": false
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.5",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kernel_info": {
      "name": "python3"
    },
    "nteract": {
      "version": "0.11.7"
    },
    "gist_id": "bfa2fb32a7fce304a24378ca31623a60"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}